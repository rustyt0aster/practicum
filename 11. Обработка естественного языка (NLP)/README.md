# [Обработка естественного языка (NLP)](https://github.com/rustyt0aster/practicum/blob/main/11.%20Обработка%20естественного%20языка%20(NLP)/Обработка%20естественного%20языка%20(NLP).ipynb)

**Цель проекта:** Обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.

| Суть проекта | Результат | Библиотеки | Инструменты и важные детали |
| :-- | :-- |:--:|:--:|
| Обработка естественного языка. Построить модель со значением метрики качества F1 не меньше 0.75. | Задача решена двумя различными методами. BERT очень точен, но сильно зависит от аппаратных возможностей оборудования. TF-IDF менее точен, но значительно быстрее может быть обработан. | Библиотеки: pandas<br>numpy<br>matplotlib.pyplot<br>torch<br>transformers<br>requests<br>lightgbm<br>nltk<br>re<br>warnings<br>pywsd<br>Модули: sklearn.model_selection (train_test_split, RandomizedSearchCV);<br>модели LogisticRegression, RandomForestClassifier, lightgbm; sklearn.metrics.f1_score; train_test_split.<br>Применяются новые модули из сторонних библиотек, вот некоторые из них: make_pipeline из sklearn.pipeline, TfidfVectorizer из sklearn.feature_extraction.text, lemmatize_sentence из pywsd.utils, word_tokenize из nltk и др. | BERT:<br>Загрузка предобученных модели и токенизатора<br>Преобразование текста - токенизация<br>Преобразование в эмбеддинги - цикл, преобразующий токены в эмбеддинги. Использует ГПУ, настроен на работу в Google Colab<br>Загрузка датасета со стороннего сайта (Я.Диска)<br>Разбиение данных и обучение моделей с перебором гиперпараметров и кросс-валидацией<br><br>TF-IDF:<br>Создание функции очистки текста<br>Создание функции лемматизатора<br>Очистка и лемматизация текста<br>Создание облака слов - наиболее часто встречающихся слов. Разбиение лемматизированного текста на токены (не одно и то же, что в BERTе) и введение стоп-слов, фильтра слов и построение графика<br>Создание мешков слов (обучение применяется только на обучающую выборку. transform - к обеим)<br>Обучение моделей. Кроме перебора гиперпараметров у моделей, а также использования кросс-валидации, в проекте используется пайплайн - во избежание утечек данных. Все модели оборачиваются в .make_pipeline() |

В работе была проведена оценка производительности разных моделей: BERT и TF-IDF. 

Преобразовав текст для BERT, получил значение метрики близкое к максимуму: единице. В то же время, модели обучались лишь на 10% данных, здесь упираемся в аппаратные ограничения и скорость обучения и перебора параметров. 
В методе TF-IDF я добился необходимого значения метрики F1. Здесь модель обучалась уже на всем наборе данных. То есть, даже если этот метод менее точен, чем BERT, он намного менее требователен к аппаратным ресурсам и намного быстрее.
Также, точность моделей сильно зависит от предобученных моделей и дополнительных преобразований.

[Вернуться](https://github.com/rustyt0aster/practicum/tree/main#readme)
